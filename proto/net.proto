/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/
syntax = "proto2";

package pblczero;

message EngineVersion {
  optional uint32 major = 1;
  optional uint32 minor = 2;
  optional uint32 patch = 3;
}

// Weights are a collection of layers that flow inputs to multiple outputs.
// The input size is defined by InputFormat enum.
// The final output sizes are defined by the OutputFormat enum.
// In between sizes are inplicit in the sizes of the data provided to the
// layers.
// For example: a fully connected with x inputs and x*y params, must have y
// outputs.
message Weights {
  message Layer {
    optional float min_val = 1;
    optional float max_val = 2;
    optional bytes params = 3;
  }

  // All layers except weights are optional. Can represent a convolution that
  // has biases, or batch normalization (with or without gammas and betas) and
  // optional relu activation.
  message ConvBlock {
    optional Layer weights = 1;
    optional Layer biases = 2;
    optional Layer bn_means = 3;
    optional Layer bn_stddivs = 4;
    optional Layer bn_gammas = 5;
    optional Layer bn_betas = 6;
    // If present specifies the filter size. If 0/missing it is assumed 3
    // before the final heads, and 1 in the final heads.
    optional int32 filter_size = 7;
    // Allows for a convolution block with no relu applied - ignored in
    // residual blocks.
    optional bool disable_relu = 8;
  }

  message SEunit {
    // Squeeze-excitation unit (https://arxiv.org/abs/1709.01507)
    // weights and biases of the two fully connected layers.
    optional Layer w1 = 1;
    optional Layer b1 = 2;
    optional Layer w2 = 3;
    optional Layer b2 = 4;
  }

  // Two convolution blocks one with relu and second without summed and passed through relu.
  // If se is present it applies its transform to the output of the second conv block.
  message Residual {
    optional ConvBlock conv1 = 1;
    optional ConvBlock conv2 = 2;
    optional SEunit se = 3;
  }

  // Applies weights as fully connected layer. Biases are optional.
  message FCLayer {
    optional Layer weights = 1;
    optional Layer biases = 2;
    optional bool use_relu = 3;
  }

  message Output {
    repeated ConvBlock outputConvs = 1;
    repeated FCLayer outputReductions = 2;
  }

  // Input convnet.
  optional ConvBlock input = 1;

  // Residual tower.
  repeated Residual residual = 2;

  // Legacy Policy head
  optional ConvBlock policy = 3;
  optional Layer ip_pol_w = 4;
  optional Layer ip_pol_b = 5;

  // Legacy Value head
  optional ConvBlock value = 6;
  optional Layer ip1_val_w = 7;
  optional Layer ip1_val_b = 8;
  optional Layer ip2_val_w = 9;
  optional Layer ip2_val_b = 10;

  optional Output policy_head = 11;
  optional Output value_head = 12;
}

message TrainingParams {
  optional uint32 training_steps = 1;
  optional float learning_rate = 2;
  optional float mse_loss = 3;
  optional float policy_loss = 4;
  optional float accuracy = 5;
  optional string lc0_params = 6;
}

message NetworkFormat {
  // Format to encode the input planes with. Used by position encoder.
  enum InputFormat {
    INPUT_UNKNOWN = 0;
    INPUT_CLASSICAL_112_PLANE = 1;
    // INPUT_WITH_COORDINATE_PLANES = 2; // Example. Uncomment/rename.
  }
  optional InputFormat input = 1;

  // Output format of the NN. Used by search code to interpret results.
  enum OutputFormat {
    OUTPUT_UNKNOWN = 0;
    OUTPUT_CLASSICAL = 1;
    // OUTPUT_WDL = 2;  // Example. Uncomment when implemented.
  }
  optional OutputFormat output = 2;

  // Network architecture. Used by backends to build the network.
  enum NetworkStructure {
    NETWORK_UNKNOWN = 0;
    NETWORK_CLASSICAL = 1;
    NETWORK_SE = 2;
    NETWORK_SE_AND_EXTENSIBLE_OUTPUT_HEADS = 3;
  }
  optional NetworkStructure network = 3;
}

message Format {
  enum Encoding {
    UNKNOWN = 0;
    LINEAR16 = 1;
  }

  optional Encoding weights_encoding = 1;
  // If network_format is missing, it's assumed to have
  // INPUT_CLASSICAL_112_PLANE / OUTPUT_CLASSICAL / NETWORK_CLASSICAL format.
  optional NetworkFormat network_format = 2;
}

message Net {
  optional fixed32 magic = 1;
  optional string license = 2;
  optional EngineVersion min_version = 3;
  optional Format format = 4;
  optional TrainingParams training_params = 5;
  optional Weights weights = 10;
}
